{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F776N1QHZ_pc"
      },
      "outputs": [],
      "source": [
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torchvision import datasets, transforms, models\n",
        "from datetime import datetime\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(output, target):\n",
        "  with torch.no_grad(): #we don't need to compute gradients for backpropagation-->just checking accuracy\n",
        "    preds = output.argmax(dim=-1)\n",
        "    return (preds == target).float().mean().item()"
      ],
      "metadata": {
        "id": "CdoKSf72afYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AVERAGE_RED = 0.4914\n",
        "AVERAGE_GREEN = 0.4822\n",
        "AVERAGE_BLUE = 0.4465\n",
        "STDDEV_RED = 0.2470\n",
        "STDDEV_GREEN = 0.2435\n",
        "STDDEV_BLUE = 0.2616\n"
      ],
      "metadata": {
        "id": "BoUXwxhPeP_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataloaders(data_dir: str, batch_size: int, num_workers: int = 2):\n",
        "  mean = (AVERAGE_RED, AVERAGE_GREEN, AVERAGE_BLUE)\n",
        "  std = (STDDEV_RED, STDDEV_GREEN, STDDEV_BLUE)\n",
        "\n",
        "  train_pipeline = transforms.Compose([\n",
        "      transforms.RandomCrop(32, padding=4),\n",
        "      transforms.RandomHorizontalFlip(),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize(mean, std)\n",
        "\n",
        "  ])\n",
        "\n",
        "  test_pipeline = transforms.Compose([\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize(mean, std)\n",
        "  ])\n",
        "\n",
        "  train = datasets.CIFAR10(root=data_dir, train=True, download=True, transform=train_pipeline)\n",
        "  test = datasets.CIFAR10(root=data_dir, train=False, download=True, transform=test_pipeline)\n",
        "\n",
        "  train_loader = torch.utils.data.DataLoader(dataset= train, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
        "  test_loader = torch.utils.data.DataLoader(dataset= test, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
        "\n",
        "  return train_loader, test_loader"
      ],
      "metadata": {
        "id": "UMWM_2rhls4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a ResNet-18 network architecture\n",
        "\n",
        "\n",
        "def build_model(num_classes:int=10):\n",
        "  model = models.resnet18(weights = None)\n",
        "  model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "  return model"
      ],
      "metadata": {
        "id": "tENTSYMkl7Ef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from contextlib import nullcontext\n",
        "\n",
        "def train_epoch(model,loader,device,criterion, optimizer, scaler=None):\n",
        "  model.train() #put model into training mode\n",
        "  loss_sum = 0.0\n",
        "  acc_sum = 0.0\n",
        "  n = 0\n",
        "\n",
        "  use_cuda_amp = (device.type == \"cuda\") and (scaler is not None)\n",
        "  autocast_ctx = amp.autocast(device_type=\"cuda\", enabled=True) if use_cuda_amp else nullcontext()\n",
        "\n",
        "  #loop through batches\n",
        "  for x, y in loader: #x = images, y = labels\n",
        "    x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
        "    #.to(device) moves data to GPU\n",
        "    #non_blocking=True => lets transfers overlap with compute\n",
        "\n",
        "    with autocast_ctx:\n",
        "      out = model(x)\n",
        "      loss = criterion(out,y)\n",
        "\n",
        "    if use_cuda_amp:\n",
        "      scaler.scale(loss).backward()\n",
        "      scaler.step(optimizer)\n",
        "      scaler.update()\n",
        "    else:\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    bs = y.size(0)\n",
        "    n += bs\n",
        "    loss_sum += loss.item() * bs\n",
        "    acc_sum += accuracy(out, y) * bs\n",
        "\n",
        "  return loss_sum/n, acc_sum/n\n",
        "\n",
        "#return loss_sum/n, acc_sum/n so the averages are weighted\n",
        "#by how many n were in each batch\n",
        "#this gives true overall loss and accuracy of the epoch"
      ],
      "metadata": {
        "id": "ncWCocIvfMLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, loader, device, criterion):\n",
        "  model.eval()\n",
        "  loss_sum = 0.0\n",
        "  acc_sum = 0.0\n",
        "  n = 0\n",
        "\n",
        "  for x, y in loader:\n",
        "    x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
        "\n",
        "    outputs = model(x)\n",
        "    loss = criterion(outputs, y)\n",
        "\n",
        "    bs = y.size(0)\n",
        "    n += bs\n",
        "    loss_sum += loss.item() * bs\n",
        "    acc_sum += accuracy(outputs,y) * bs\n",
        "\n",
        "  epoch_loss = loss_sum / n\n",
        "  epoch_acc = acc_sum / n\n",
        "\n",
        "  return epoch_loss, epoch_acc"
      ],
      "metadata": {
        "id": "r_BGnAh4jh7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model, out_dir):\n",
        "  #defines a helper function to save trained model in PyTorch and TorchScript\n",
        "  os.makedirs(out_dir, exist_ok=True)\n",
        "  model_path = os.path.join\n",
        "\n",
        "  stamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "  ckpt = os.path.join(out_dir, f\"resnet18_cifar10_{stamp}.pth\")\n",
        "  #had to search up how to create a unique timestamp and bui  ld checkpoint path\n",
        "\n",
        "  torch.save(model.state_dict(), ckpt)\n",
        "  #saves model's state_dict (all learned weights/biases)\n",
        "\n",
        "  example = torch.randn(1,3,32,32,\n",
        "                        device=next(model.parameters()).device)\n",
        "  #make a dummy input tensor shaped like CIFAR-10 Data (1 image, 3 channels, 32x32)\n",
        "\n",
        "  ts = torch.jit.trace(model.eval(), example)\n",
        "    #convert model into TorchScript\n",
        "    #TorchScript = serialized, optimized version of model\n",
        "    #makes it portable to environments w/o Python (C++)\n",
        "  ts_path = ckpt.replace(\".pth\", \".ts.pt\")\n",
        "    #build a filename for TorchScript export\n",
        "  ts.save(ts_path)\n",
        "    #save TorchScript model to disk\n",
        "  return ckpt, ts_path\n",
        "    #returns both file paths\n",
        "\n"
      ],
      "metadata": {
        "id": "crB0LN9Rkgzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import amp\n",
        "#Time to train the ResNet-18 on CIFAR-10\n",
        "\n",
        "\n",
        "EPOCHS = 60\n",
        " #passes through 50k images in training set 20 times\n",
        "BATCH_SIZE = 128 #start w/ 64 or 128 rule of thumb. How many images go into model before weights updated\n",
        "LR = 0.1 # learning-rate (step size for weight updates)\n",
        "WEIGHT_DECAY = 5e-4 #technique to prevent overfitting model regularization parameter (lambda)\n",
        "\n",
        "#'cuda' = GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"using device: {device}\")\n",
        "\n",
        "train_loader, test_loader = get_dataloaders(\"/content/data\", batch_size = BATCH_SIZE, num_workers = 2)\n",
        "model = build_model()\n",
        "model.to(device) #moves all model weights onto that device\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "#loss function for multi-classification\n",
        "\n",
        "#Stochastic Gradient Descent: unlike traditional gradient descent\n",
        "#which uses entire dataset to compute the gradient, SGD updates the model\n",
        "#parameters using only a single randomly selected training example of batch\n",
        "#allows for randomness into the optimization process ==> \"Stochastic\"\n",
        "optimizer = optim.SGD(model.parameters(),lr=LR,momentum =0.9,weight_decay=WEIGHT_DECAY)\n",
        "#model.parameters() = all the numbers the model is allowed to change during training\n",
        "\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "#cosine shape makes smooth decay in scheduling learning rate (big -> small)\n",
        "\n",
        "#Other Schedules:\n",
        "  # - StepLR: drops LR by a factor every N epochs (piecewise constant)\n",
        "  # - ExponentialLR: decays LR exponentially\n",
        "  # - ReduceLROnPlateau: lowers LR when validation accuracy stops improving\n",
        "  # - CosineAnnealingLR: smooth,gradual, widely used for ResNets on CIFAR/ImageNet\n",
        "\n",
        "scaler = amp.GradScaler(device='cuda', enabled=(device.type == \"cuda\"))\n",
        "\n",
        "\n",
        "best_accuracy = 0.0\n",
        "best_paths = (None, None)\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "  tr_loss, tr_acc = train_epoch(model, train_loader, device,\n",
        "                                    criterion, optimizer, scaler)\n",
        "  #run one full training pass: forward -> loss -> backward -> optimizer step\n",
        "  va_loss, va_acc = evaluate(model, test_loader, device, criterion)\n",
        "  #run eval on test loader: no gradients, returns avg validation loss/accuracy\n",
        "  scheduler.step()\n",
        "  #updates (reduces) learning rate for next epoch\n",
        "\n",
        "\n",
        "  if va_acc > best_accuracy:\n",
        "    best_accuracy = va_acc\n",
        "    ckpt, ts_path = save_model(model, \"/content/artifacts\")  # <-- ensure function name matches your helpers\n",
        "    best_paths = (ckpt, ts_path)\n",
        "    print(\"Saved BEST:\", ckpt)\n",
        "\n",
        "  #formatted progress log\n",
        "  print(f\"Epoch {epoch:03d} | train_loss {tr_loss:.4f} acc {tr_acc:.4f} | val_loss {va_loss:.4f} acc {va_acc:.4f} | best {best_accuracy:.4f}\")\n",
        "\n",
        "\n",
        "print(\"Best val_acc:\", best_accuracy)\n",
        "print(\"Best artifacts:\", best_paths)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLoYg3cUmkOR",
        "outputId": "84b548b4-377e-4706-ce01-a76e6efc45b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:04<00:00, 39.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved BEST: /content/artifacts/resnet18_cifar10_20260115_041418.pth\n",
            "Epoch 001 | train_loss 2.2011 acc 0.2919 | val_loss 1.8362 acc 0.4067 | best 0.4067\n",
            "Saved BEST: /content/artifacts/resnet18_cifar10_20260115_041442.pth\n",
            "Epoch 002 | train_loss 1.6982 acc 0.4406 | val_loss 1.5922 acc 0.4892 | best 0.4892\n",
            "Saved BEST: /content/artifacts/resnet18_cifar10_20260115_041504.pth\n",
            "Epoch 003 | train_loss 1.5261 acc 0.5254 | val_loss 1.3748 acc 0.6015 | best 0.6015\n",
            "Saved BEST: /content/artifacts/resnet18_cifar10_20260115_041527.pth\n",
            "Epoch 004 | train_loss 1.3949 acc 0.5941 | val_loss 1.2924 acc 0.6411 | best 0.6411\n",
            "Saved BEST: /content/artifacts/resnet18_cifar10_20260115_041550.pth\n",
            "Epoch 005 | train_loss 1.3210 acc 0.6304 | val_loss 1.2534 acc 0.6582 | best 0.6582\n",
            "Epoch 006 | train_loss 1.2648 acc 0.6561 | val_loss 1.2758 acc 0.6564 | best 0.6582\n",
            "Saved BEST: /content/artifacts/resnet18_cifar10_20260115_041637.pth\n",
            "Epoch 007 | train_loss 1.2195 acc 0.6793 | val_loss 1.2303 acc 0.6742 | best 0.6742\n",
            "Saved BEST: /content/artifacts/resnet18_cifar10_20260115_041700.pth\n",
            "Epoch 008 | train_loss 1.1936 acc 0.6902 | val_loss 1.1960 acc 0.6911 | best 0.6911\n",
            "Saved BEST: /content/artifacts/resnet18_cifar10_20260115_041722.pth\n",
            "Epoch 009 | train_loss 1.1690 acc 0.7020 | val_loss 1.1380 acc 0.7115 | best 0.7115\n",
            "Saved BEST: /content/artifacts/resnet18_cifar10_20260115_041746.pth\n",
            "Epoch 010 | train_loss 1.1433 acc 0.7138 | val_loss 1.1098 acc 0.7276 | best 0.7276\n",
            "Epoch 011 | train_loss 1.1253 acc 0.7222 | val_loss 1.1897 acc 0.6945 | best 0.7276\n",
            "Saved BEST: /content/artifacts/resnet18_cifar10_20260115_041832.pth\n",
            "Epoch 012 | train_loss 1.1111 acc 0.7285 | val_loss 1.0992 acc 0.7323 | best 0.7323\n",
            "Epoch 013 | train_loss 1.0956 acc 0.7364 | val_loss 1.1817 acc 0.7068 | best 0.7323\n",
            "Epoch 014 | train_loss 1.0806 acc 0.7449 | val_loss 1.1807 acc 0.7011 | best 0.7323\n",
            "Epoch 015 | train_loss 1.0715 acc 0.7479 | val_loss 1.1197 acc 0.7277 | best 0.7323\n",
            "Saved BEST: /content/artifacts/resnet18_cifar10_20260115_042002.pth\n",
            "Epoch 016 | train_loss 1.0548 acc 0.7538 | val_loss 1.0643 acc 0.7535 | best 0.7535\n",
            "Epoch 017 | train_loss 1.0502 acc 0.7565 | val_loss 1.1152 acc 0.7304 | best 0.7535\n",
            "Saved BEST: /content/artifacts/resnet18_cifar10_20260115_042048.pth\n",
            "Epoch 018 | train_loss 1.0377 acc 0.7616 | val_loss 1.0466 acc 0.7581 | best 0.7581\n",
            "Epoch 019 | train_loss 1.0282 acc 0.7662 | val_loss 1.0969 acc 0.7395 | best 0.7581\n",
            "Saved BEST: /content/artifacts/resnet18_cifar10_20260115_042133.pth\n",
            "Epoch 020 | train_loss 1.0224 acc 0.7682 | val_loss 1.0462 acc 0.7626 | best 0.7626\n",
            "Saved BEST: /content/artifacts/resnet18_cifar10_20260115_042156.pth\n",
            "Epoch 021 | train_loss 1.0061 acc 0.7767 | val_loss 0.9974 acc 0.7866 | best 0.7866\n",
            "Epoch 022 | train_loss 0.9828 acc 0.7873 | val_loss 1.0040 acc 0.7831 | best 0.7866\n",
            "Epoch 023 | train_loss 0.9825 acc 0.7874 | val_loss 0.9990 acc 0.7850 | best 0.7866\n",
            "Epoch 024 | train_loss 0.9845 acc 0.7855 | val_loss 0.9982 acc 0.7852 | best 0.7866\n",
            "Epoch 025 | train_loss 0.9846 acc 0.7869 | val_loss 1.0007 acc 0.7844 | best 0.7866\n",
            "Epoch 026 | train_loss 0.9849 acc 0.7882 | val_loss 0.9995 acc 0.7837 | best 0.7866\n",
            "Epoch 027 | train_loss 0.9857 acc 0.7857 | val_loss 1.0032 acc 0.7834 | best 0.7866\n",
            "Epoch 028 | train_loss 0.9828 acc 0.7884 | val_loss 0.9976 acc 0.7857 | best 0.7866\n",
            "Epoch 029 | train_loss 0.9815 acc 0.7892 | val_loss 0.9989 acc 0.7858 | best 0.7866\n",
            "Epoch 030 | train_loss 0.9888 acc 0.7828 | val_loss 1.0025 acc 0.7846 | best 0.7866\n",
            "Epoch 031 | train_loss 0.9859 acc 0.7849 | val_loss 1.0039 acc 0.7839 | best 0.7866\n",
            "Epoch 032 | train_loss 0.9812 acc 0.7886 | val_loss 1.0047 acc 0.7826 | best 0.7866\n",
            "Epoch 033 | train_loss 0.9868 acc 0.7851 | val_loss 1.0032 acc 0.7833 | best 0.7866\n",
            "Epoch 034 | train_loss 0.9820 acc 0.7877 | val_loss 1.0035 acc 0.7839 | best 0.7866\n",
            "Epoch 035 | train_loss 0.9838 acc 0.7854 | val_loss 1.0008 acc 0.7847 | best 0.7866\n",
            "Epoch 036 | train_loss 0.9826 acc 0.7869 | val_loss 1.0027 acc 0.7841 | best 0.7866\n",
            "Epoch 037 | train_loss 0.9849 acc 0.7864 | val_loss 0.9967 acc 0.7857 | best 0.7866\n",
            "Epoch 038 | train_loss 0.9869 acc 0.7843 | val_loss 1.0026 acc 0.7834 | best 0.7866\n",
            "Epoch 039 | train_loss 0.9851 acc 0.7861 | val_loss 1.0053 acc 0.7843 | best 0.7866\n",
            "Epoch 040 | train_loss 0.9862 acc 0.7861 | val_loss 1.0020 acc 0.7836 | best 0.7866\n",
            "Epoch 041 | train_loss 0.9854 acc 0.7872 | val_loss 0.9999 acc 0.7852 | best 0.7866\n",
            "Epoch 042 | train_loss 0.9844 acc 0.7861 | val_loss 1.0016 acc 0.7848 | best 0.7866\n",
            "Epoch 043 | train_loss 0.9872 acc 0.7854 | val_loss 1.0038 acc 0.7832 | best 0.7866\n",
            "Epoch 044 | train_loss 0.9835 acc 0.7866 | val_loss 0.9986 acc 0.7855 | best 0.7866\n",
            "Epoch 045 | train_loss 0.9804 acc 0.7891 | val_loss 0.9991 acc 0.7848 | best 0.7866\n",
            "Epoch 046 | train_loss 0.9835 acc 0.7857 | val_loss 0.9988 acc 0.7850 | best 0.7866\n",
            "Epoch 047 | train_loss 0.9861 acc 0.7859 | val_loss 0.9990 acc 0.7845 | best 0.7866\n",
            "Epoch 048 | train_loss 0.9857 acc 0.7858 | val_loss 1.0036 acc 0.7839 | best 0.7866\n",
            "Epoch 049 | train_loss 0.9829 acc 0.7872 | val_loss 1.0010 acc 0.7838 | best 0.7866\n",
            "Epoch 050 | train_loss 0.9857 acc 0.7861 | val_loss 1.0009 acc 0.7847 | best 0.7866\n",
            "Epoch 051 | train_loss 0.9870 acc 0.7852 | val_loss 0.9984 acc 0.7844 | best 0.7866\n",
            "Epoch 052 | train_loss 0.9839 acc 0.7865 | val_loss 1.0019 acc 0.7840 | best 0.7866\n",
            "Epoch 053 | train_loss 0.9836 acc 0.7876 | val_loss 1.0029 acc 0.7847 | best 0.7866\n",
            "Epoch 054 | train_loss 0.9875 acc 0.7850 | val_loss 0.9984 acc 0.7853 | best 0.7866\n",
            "Epoch 055 | train_loss 0.9854 acc 0.7848 | val_loss 0.9990 acc 0.7857 | best 0.7866\n",
            "Epoch 056 | train_loss 0.9851 acc 0.7856 | val_loss 0.9976 acc 0.7857 | best 0.7866\n",
            "Epoch 057 | train_loss 0.9784 acc 0.7902 | val_loss 1.0016 acc 0.7842 | best 0.7866\n",
            "Epoch 058 | train_loss 0.9873 acc 0.7842 | val_loss 1.0030 acc 0.7838 | best 0.7866\n",
            "Epoch 059 | train_loss 0.9864 acc 0.7866 | val_loss 1.0069 acc 0.7825 | best 0.7866\n",
            "Epoch 060 | train_loss 0.9842 acc 0.7872 | val_loss 1.0053 acc 0.7825 | best 0.7866\n",
            "Best val_acc: 0.7866\n",
            "Best artifacts: ('/content/artifacts/resnet18_cifar10_20260115_042156.pth', '/content/artifacts/resnet18_cifar10_20260115_042156.ts.pt')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import io, copy, time\n",
        "import torch.nn.utils.prune as prune\n",
        "\n",
        "# Use this for pruning fine-tune and training too\n",
        "\n",
        "def train_epoch_fixed(model, loader, device, criterion, optimizer, scaler=None):\n",
        "    model.train()\n",
        "    loss_sum, acc_sum, n = 0.0, 0.0, 0\n",
        "\n",
        "    use_cuda_amp = (device.type == \"cuda\") and (scaler is not None)\n",
        "    autocast_ctx = amp.autocast(device_type=\"cuda\", enabled=True) if use_cuda_amp else nullcontext()\n",
        "\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with autocast_ctx:\n",
        "            out = model(x)\n",
        "            loss = criterion(out, y)\n",
        "\n",
        "        if use_cuda_amp:\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        bs = y.size(0)\n",
        "        n += bs\n",
        "        loss_sum += loss.item() * bs\n",
        "        # assumes your accuracy(output, target) exists\n",
        "        acc_sum += accuracy(out, y) * bs\n",
        "\n",
        "    return loss_sum / n, acc_sum / n\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Utilities: size + latency\n",
        "# -----------------------------\n",
        "def model_size_mb_via_torchsave(model) -> float:\n",
        "    buf = io.BytesIO()\n",
        "    torch.save(model.state_dict(), buf)\n",
        "    return len(buf.getbuffer()) / (1024 ** 2)\n",
        "\n",
        "@torch.inference_mode()\n",
        "def benchmark_cpu_latency_ms(model, iters=200, warmup=50, batch_size=1, num_threads=None):\n",
        "    model = model.eval().cpu()\n",
        "    if num_threads is not None:\n",
        "        torch.set_num_threads(int(num_threads))\n",
        "\n",
        "    x = torch.randn(batch_size, 3, 32, 32)  # CIFAR-10 shape\n",
        "    # warmup\n",
        "    for _ in range(warmup):\n",
        "        _ = model(x)\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    for _ in range(iters):\n",
        "        _ = model(x)\n",
        "    t1 = time.perf_counter()\n",
        "\n",
        "    return (t1 - t0) * 1000.0 / iters\n",
        "\n",
        "\n",
        "# Load best FP32 checkpoint\n",
        "\n",
        "BEST_CKPT_PATH = '/content/artifacts/resnet18_cifar10_20260115_042156.pth'\n",
        "\n",
        "device_train = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device_cpu = torch.device(\"cpu\")\n",
        "\n",
        "# Use your existing loader helper\n",
        "train_loader, test_loader = get_dataloaders(\"/content/data\", batch_size=128, num_workers=2)\n",
        "\n",
        "# Rebuild + load weights\n",
        "model_fp32 = build_model()\n",
        "model_fp32.load_state_dict(torch.load(BEST_CKPT_PATH, map_location=\"cpu\"))\n",
        "model_fp32 = model_fp32.to(device_train)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "# Baseline eval (on CPU for apples-to-apples with PTQ)\n",
        "model_fp32_cpu = copy.deepcopy(model_fp32).eval().cpu()\n",
        "base_loss, base_acc = evaluate(model_fp32_cpu, test_loader, device_cpu, criterion)\n",
        "\n",
        "base_size = model_size_mb_via_torchsave(model_fp32_cpu)\n",
        "base_lat = benchmark_cpu_latency_ms(model_fp32_cpu, iters=200, warmup=50, batch_size=1, num_threads=1)\n",
        "\n",
        "print(f\"[BASE FP32] acc={base_acc:.4f} loss={base_loss:.4f} size={base_size:.1f}MB latency={base_lat:.2f}ms (1 thread)\")\n",
        "\n",
        "\n",
        "# A) PTQ INT8 (FX graph mode) + accuracy + size + latency\n",
        "# Notes:\n",
        "# - Static PTQ runs on CPU.\n",
        "# - For best results, calibrate on a few thousand real images (no grads).\n",
        "\n",
        "# Quantization imports (handles minor API moves)\n",
        "try:\n",
        "    from torch.ao.quantization import QConfigMapping, get_default_qconfig\n",
        "    from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx\n",
        "except Exception:\n",
        "    from torch.ao.quantization import QConfigMapping, get_default_qconfig\n",
        "    from torch.quantization.quantize_fx import prepare_fx, convert_fx\n",
        "\n",
        "# Pick quantized engine\n",
        "supported = torch.backends.quantized.supported_engines\n",
        "engine = \"fbgemm\" if \"fbgemm\" in supported else (\"qnnpack\" if \"qnnpack\" in supported else supported[0])\n",
        "torch.backends.quantized.engine = engine\n",
        "print(\"Quant engine:\", engine)\n",
        "\n",
        "model_to_quantize = copy.deepcopy(model_fp32_cpu).eval()\n",
        "\n",
        "# QConfigMapping for PTQ\n",
        "qconfig = get_default_qconfig(engine)\n",
        "qconfig_mapping = QConfigMapping().set_global(qconfig)\n",
        "\n",
        "example_inputs = (torch.randn(1, 3, 32, 32),)\n",
        "\n",
        "prepared = prepare_fx(model_to_quantize, qconfig_mapping, example_inputs)\n",
        "\n",
        "@torch.inference_mode()\n",
        "def calibrate(prepared_model, loader, num_batches=100):\n",
        "    prepared_model.eval()\n",
        "    for i, (x, _) in enumerate(loader):\n",
        "        x = x.to(\"cpu\")\n",
        "        _ = prepared_model(x)\n",
        "        if i + 1 >= num_batches:\n",
        "            break\n",
        "\n",
        "calibrate(prepared, test_loader, num_batches=100)  # ~12.8k images if batch=128\n",
        "\n",
        "model_int8 = convert_fx(prepared).eval().cpu()\n",
        "\n",
        "int8_loss, int8_acc = evaluate(model_int8, test_loader, device_cpu, criterion)\n",
        "int8_size = model_size_mb_via_torchsave(model_int8)\n",
        "int8_lat = benchmark_cpu_latency_ms(model_int8, iters=200, warmup=50, batch_size=1, num_threads=1)\n",
        "\n",
        "print(f\"[PTQ INT8 FX] acc={int8_acc:.4f} loss={int8_loss:.4f} size={int8_size:.1f}MB latency={int8_lat:.2f}ms (1 thread)\")\n",
        "print(f\"  Size shrink: {base_size/int8_size:.2f}x  |  Latency speedup: {base_lat/int8_lat:.2f}x  |  Acc drop: {(base_acc-int8_acc)*100:.2f} pts\")\n",
        "\n",
        "# Optional: save artifacts\n",
        "os.makedirs(\"/content/artifacts_ptq\", exist_ok=True)\n",
        "torch.save(model_int8.state_dict(), \"/content/artifacts_ptq/resnet18_cifar10_int8_fx_state_dict.pth\")\n",
        "\n",
        "\n",
        "# B) Structured pruning + fine-tune\n",
        "# NOTE: torch.nn.utils.prune creates masks (structured sparsity).\n",
        "# \"Effective nonzero params\" drop ~30%, but tensor shapes do NOT shrink unless you do channel-slimming surgery.\n",
        "\n",
        "def apply_structured_pruning_resnet(model, amount=0.30):\n",
        "    # Prune output channels (dim=0) for Conv2d and output features for Linear\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            prune.ln_structured(m, name=\"weight\", amount=amount, n=2, dim=0)\n",
        "        elif isinstance(m, nn.Linear):\n",
        "            prune.ln_structured(m, name=\"weight\", amount=amount, n=2, dim=0)\n",
        "    return model\n",
        "\n",
        "def effective_nonzero_params(model):\n",
        "    # counts weights AFTER applying masks (if present)\n",
        "    total, nonzero = 0, 0\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
        "            if hasattr(m, \"weight_mask\") and hasattr(m, \"weight_orig\"):\n",
        "                w = (m.weight_orig.detach() * m.weight_mask.detach())\n",
        "            else:\n",
        "                w = m.weight.detach()\n",
        "            total += w.numel()\n",
        "            nonzero += torch.count_nonzero(w).item()\n",
        "            if m.bias is not None:\n",
        "                b = m.bias.detach()\n",
        "                total += b.numel()\n",
        "                nonzero += torch.count_nonzero(b).item()\n",
        "    return nonzero, total\n",
        "\n",
        "# Start from FP32 weights on training device\n",
        "model_pruned = copy.deepcopy(model_fp32).to(device_train)\n",
        "\n",
        "# Apply pruning\n",
        "model_pruned = apply_structured_pruning_resnet(model_pruned, amount=0.30)\n",
        "\n",
        "nz, tot = effective_nonzero_params(model_pruned)\n",
        "print(f\"[PRUNE] effective nonzero params: {nz:,}/{tot:,}  (sparsity={(1-nz/tot)*100:.2f}%)\")\n",
        "\n",
        "# Fine-tune (small LR, few epochs)\n",
        "FT_EPOCHS = 20\n",
        "FT_LR = 0.01\n",
        "\n",
        "optimizer_ft = optim.SGD(model_pruned.parameters(), lr=FT_LR, momentum=0.9, weight_decay=5e-4)\n",
        "scheduler_ft = CosineAnnealingLR(optimizer_ft, T_max=FT_EPOCHS)\n",
        "scaler_ft = amp.GradScaler(device='cuda', enabled=(device_train.type == \"cuda\"))\n",
        "\n",
        "best_acc = 0.0\n",
        "best_sd = None\n",
        "\n",
        "for epoch in range(1, FT_EPOCHS + 1):\n",
        "    tr_loss, tr_acc = train_epoch_fixed(model_pruned, train_loader, device_train, criterion, optimizer_ft, scaler_ft)\n",
        "    va_loss, va_acc = evaluate(model_pruned.eval(), test_loader, device_train, criterion)\n",
        "    scheduler_ft.step()\n",
        "\n",
        "    if va_acc > best_acc:\n",
        "        best_acc = va_acc\n",
        "        best_sd = copy.deepcopy(model_pruned.state_dict())\n",
        "\n",
        "    print(f\"[FT {epoch:02d}/{FT_EPOCHS}] train_acc={tr_acc:.4f} val_acc={va_acc:.4f} best_val_acc={best_acc:.4f}\")\n",
        "\n",
        "# Load best fine-tuned pruned weights\n",
        "if best_sd is not None:\n",
        "    model_pruned.load_state_dict(best_sd)\n",
        "\n",
        "# OPTIONAL: make pruning permanent (removes reparam, keeps zeros)\n",
        "for m in model_pruned.modules():\n",
        "    if isinstance(m, (nn.Conv2d, nn.Linear)) and hasattr(m, \"weight_mask\"):\n",
        "        prune.remove(m, \"weight\")\n",
        "\n",
        "# Final pruned accuracy (CPU eval if you want to compare like PTQ)\n",
        "model_pruned_cpu = copy.deepcopy(model_pruned).eval().cpu()\n",
        "pr_loss, pr_acc = evaluate(model_pruned_cpu, test_loader, device_cpu, criterion)\n",
        "\n",
        "nz2, tot2 = effective_nonzero_params(model_pruned_cpu)  # after prune.remove, should still reflect zeros\n",
        "print(f\"[PRUNED+FT] acc={pr_acc:.4f} loss={pr_loss:.4f} | effective sparsity={(1-nz2/tot2)*100:.2f}% | acc drop={(base_acc-pr_acc)*100:.2f} pts\")\n",
        "\n",
        "# Optional save\n",
        "os.makedirs(\"/content/artifacts_pruned\", exist_ok=True)\n",
        "torch.save(model_pruned_cpu.state_dict(), \"/content/artifacts_pruned/resnet18_cifar10_pruned_ft_state_dict.pth\")\n"
      ],
      "metadata": {
        "id": "MPZOS-1D2_22",
        "outputId": "080ee7c7-bf67-4839-d4bd-122ce9176fee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BASE FP32] acc=0.7866 loss=0.9974 size=42.7MB latency=8.69ms (1 thread)\n",
            "Quant engine: fbgemm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2042866671.py:119: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  prepared = prepare_fx(model_to_quantize, qconfig_mapping, example_inputs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-2042866671.py:132: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  model_int8 = convert_fx(prepared).eval().cpu()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PTQ INT8 FX] acc=0.7891 loss=0.9969 size=10.8MB latency=2.59ms (1 thread)\n",
            "  Size shrink: 3.96x  |  Latency speedup: 3.36x  |  Acc drop: -0.25 pts\n",
            "[PRUNE] effective nonzero params: 7,814,369/11,172,042  (sparsity=30.05%)\n",
            "[FT 01/20] train_acc=0.6876 val_acc=0.6910 best_val_acc=0.6910\n",
            "[FT 02/20] train_acc=0.7016 val_acc=0.7029 best_val_acc=0.7029\n",
            "[FT 03/20] train_acc=0.7080 val_acc=0.7050 best_val_acc=0.7050\n",
            "[FT 04/20] train_acc=0.7138 val_acc=0.7003 best_val_acc=0.7050\n",
            "[FT 05/20] train_acc=0.7161 val_acc=0.6972 best_val_acc=0.7050\n",
            "[FT 06/20] train_acc=0.7202 val_acc=0.7002 best_val_acc=0.7050\n",
            "[FT 07/20] train_acc=0.7207 val_acc=0.7110 best_val_acc=0.7110\n",
            "[FT 08/20] train_acc=0.7212 val_acc=0.7072 best_val_acc=0.7110\n",
            "[FT 09/20] train_acc=0.7254 val_acc=0.7120 best_val_acc=0.7120\n",
            "[FT 10/20] train_acc=0.7309 val_acc=0.7157 best_val_acc=0.7157\n",
            "[FT 11/20] train_acc=0.7305 val_acc=0.7180 best_val_acc=0.7180\n",
            "[FT 12/20] train_acc=0.7390 val_acc=0.7112 best_val_acc=0.7180\n",
            "[FT 13/20] train_acc=0.7365 val_acc=0.7113 best_val_acc=0.7180\n",
            "[FT 14/20] train_acc=0.7396 val_acc=0.7132 best_val_acc=0.7180\n",
            "[FT 15/20] train_acc=0.7416 val_acc=0.7141 best_val_acc=0.7180\n",
            "[FT 16/20] train_acc=0.7478 val_acc=0.7169 best_val_acc=0.7180\n",
            "[FT 17/20] train_acc=0.7448 val_acc=0.7165 best_val_acc=0.7180\n",
            "[FT 18/20] train_acc=0.7455 val_acc=0.7166 best_val_acc=0.7180\n",
            "[FT 19/20] train_acc=0.7475 val_acc=0.7173 best_val_acc=0.7180\n",
            "[FT 20/20] train_acc=0.7488 val_acc=0.7177 best_val_acc=0.7180\n",
            "[PRUNED+FT] acc=0.7180 loss=1.0298 | effective sparsity=30.05% | acc drop=6.86 pts\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rcz73Led-hiW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}